# 引子-线性回归
## 线性回归模型
线性回归模型是回归分析中最基础的模型，其基本形式为：
$$y=b+wx$$
以房价预测为例，$y$为房价，$x$为面积，$b$为截距，$w$为斜率。
如何确定$b$和$w$，让房价预测的误差尽可能小？
这个问题可以转化为求最小方差
$$min\sum_{i=1}^n(y_i-\hat{y_i})^2$$
$\hat{y_i}$为预测值，$y_i$为真实值。
其中$\hat{y_i}$可以通过$b$和$w$来确定，即$\hat{y_i}=b+wx_i$。
上面的最小方差公式可以转化为
$$min\sum_{i=1}^n(y_i-b-wx_i)^2$$
由于$x$和$y$都是已知的，所以求最小方差的问题可以转化为求$w$与$b$的值
将上面最小方差除以2n，得到下面的公式
$$J(w,b)=\frac{1}{2m}\sum_{i=1}^n(y_i-b-wx_i)^2$$
将$J(w,b)$对$w$称为成本函数。
* **代价函数（loss function）与成本函数（cost function）**
  
线性回归的代价函数是一个三维的图型，求三维图形的最低点即为最优解。
* 代价函数衡量的是单个训练样本的预测值与真实值之间的差异，用于评估模型在一个单一数据点上的表现。
* 成本函数是对整个训练集上所有样本的代价函数的平均值，或者是某种形式的聚合，用于评估模型在整个训练集上的整体表现。

成本函数计算过程
![替代文本](cost-function.png)


## 梯度下降法
梯度下降法是一种常用的求解最优解的方法，其基本思想是：
1. 初始化参数$w$和$b$，一般初始化为0。
2. 计算梯度，即求出$J(w,b)$关于$w$和$b$的偏导数。
3. 更新参数，即$w=w-\alpha\frac{\partial J}{\partial w}$，$b=b-\alpha\frac{\partial J}{\partial b}$。
其中，$\alpha$为学习率，用于控制步长。学习率通常是一个在0到1之间的数值，例如0.01。

上面计算$w$与$b$的更新值是同时进行的，计算过程如下：
首先计算$w$的更新值$tempw$ ,然后计算$b$的更新值$tempb$，最后，将$tmpw$赋值给$w$ ，将$tempb$赋值给$b$。
**总结1：**
梯度下降算法就是通过偏导数与学习率$alpha$来确定每次更新的参数，并将计算出来的参数运用在下一轮计算中

**线性回归在梯度下降的过程中只会有一个局部最优点，因为线性回归的成本函数是一个凸函数，所以一定能找到全局最优点，而如果成本函数不是凸函数，则不一定能找到全局最优点。**

### 批量梯度下降
* 批量梯度下降（Batch Gradient Descent）是一种常用的优化算法，用于最小化机器学习模型的成本函数。与随机梯度下降（Stochastic Gr adient Descent, SGD）和小批量梯度下降（Mini-Batch Gradient Descent）不同，批量梯度下降在每次迭代中使用整个训练集来更新模型参数，批量梯度下降收敛稳定，可利用矩阵进行高效的计算，在数据集很大时，计算成本较高。
* 小批量梯度下降（Mini-Batch Gradient Descent, MBGD） 每次使用一个固定数量的样本计算梯度并更新参数，损失函数对小批量样本求和或平均，然后计算梯度并更新权重 
* 随机梯度下降（Stochastic Gradient Descent, SGD） 每次仅用一个训练样本来计算梯度并更新权重，损失函数对单个样本求导，然后立即更新权重

## 学习率$\alpha$
* 如果学习率太小，会导致学习效率降低，计算成本变高
* 如果学习率过大，学习步长过大，可能会导致无法到达最优解，或者进入局部最优解。
* 当出现局部最小值时，梯度下降的步长会变得很小，会逐步衰减为0，最终到达局部最优点。为避免学习过程中一直处于局部最优点，可以采取一下措施：
1. 随机初始化：
多次运行梯度下降算法，每次使用不同的初始权重。这样可以增加找到全局最小值的机会。
2. 动量（Momentum）：
在梯度下降过程中加入动量项，可以帮助算法更快地穿越平坦区域，并有助于跳出浅的局部最小值。动量项通过累积过去梯度的方向，使得更新方向更加稳定。
3. 自适应学习率方法：
使用自适应学习率的方法，如Adam、Adagrad、RMSprop等，这些方法可以根据参数的历史梯度信息动态调整学习率，有助于避免局部最小值。
4. 学习率衰减：
随着训练的进行逐渐降低学习率，可以帮助模型在训练初期快速逃离局部最小值，而在后期更加精细地逼近最优解。
5. 正则化：
引入正则化项（如L1或L2正则化）可以帮助模型避免过拟合，同时也有助于减少陷入局部最小值的风险。
6. 增加模型复杂度：
有时候，增加模型的复杂度（例如添加更多层或节点）可以使损失函数的形状发生变化，从而减少局部最小值的存在。
7. 随机搜索和贝叶斯优化：
对于高维优化问题，可以使用随机搜索或贝叶斯优化等高级搜索策略来寻找更好的初始点或优化路径。
8. 遗传算法和其他进化算法：
这些算法通过模拟自然选择的过程，可以有效地探索搜索空间，避免陷入局部最小值。

## 逻辑回归函数
逻辑回归函数（Logistic Regression）是一种用于分类问题的机器学习算法，用于预测一个样本属于某一类别的概率。
逻辑回归函数公式为：
$$h_\theta(x)=\frac{1}{1+e^{-\theta^T x}}$$
逻辑回归函数的输出是一个概率值，范围在0到1之间，可以方便的用于分类问题

逻辑回归函数的成本函数为：
$$J(\theta)=\frac{1}{m}\sum_{i=1}^m[-y_i\log(h_\theta(x_i))-(1-y_i)\log(1-h_\theta(x_i))]$$
这里与线性函数使用方差的二分之一来计算成本函数不同，使用这个成本函数能保证逻辑回归的代价函数为一个突函数，避免局部最优解的问题。
逻辑回归函数的梯度下降求解公式为：
$$ \frac{\partial J}{\partial \theta_j}=\frac{1}{m}\sum_{i=1}^m(h_\theta(x_i)-y_i)x_i^j$$
看起来与线性回归的求解公式类似，而公式中的$h_\theta(x_i)$与线性回归的$y_i$不同，$h_\theta(x_i)$为逻辑回归函数的输出，$y_i$为样本的标签，$y_i$的值为0或1，$h_\theta(x_i)$的值为0到1之间，$y_i$的值为0或1，所以$h_\theta(x_i)-y_i$的值为0或1。
通过python代码实现逻辑回归函数的梯度下降求解：
```python
import numpy as np

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Cost function for logistic regression
def cost_function(X, y, weights):
    m = len(y)
    h = sigmoid(np.dot(X, weights))
    # 成本函数
    cost = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    # 梯度
    gradient = (1/m) * np.dot(X.T, (h - y))
    return cost, gradient

# Gradient descent to minimize the cost function
def gradient_descent(X, y, weights, learning_rate, iterations):
    m = len(y)
    costs = []
    for i in range(iterations):
        cost, gradient = cost_function(X, y, weights)
        weights = weights - (learning_rate * gradient)
        costs.append(cost)
        if i % 100 == 0:
            print(f"Iteration {i}, Cost: {cost}")
    return weights, costs

# Predict function
def predict(X, weights):
    probabilities = sigmoid(np.dot(X, weights))
    return [1 if p >= 0.5 else 0 for p in probabilities]

# Example usage:
if __name__ == "__main__":
    # Assume X is your feature matrix and y are the labels
    # You need to add a column of ones to X for the bias term
    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # Example data
    y = np.array([0, 0, 1, 1])  # Example labels
    
    # Initialize weights (including bias term)
    weights = np.zeros(X.shape[1])

    # Set hyperparameters
    learning_rate = 0.01
    iterations = 1000

    # Train the model
    final_weights, costs = gradient_descent(X, y, weights, learning_rate, iterations)

    # Make predictions
    predictions = predict(X, final_weights)
    
    print("Final Weights:", final_weights)
    print("Predictions:", predictions)
```

## bais (偏差)和（variance）方差
* 偏差（bais): 偏差是指模型预测值的期望与真实值之间的差异。高偏差意味着模型对训练数据的学习不足，它未能捕捉到数据中的有用信息，导致预测结果系统性地偏离实际值。这种情况通常出现在过于简单的模型中，即模型没有足够的灵活性来适应数据的真实趋势，从而造成欠拟合（underfitting）。欠拟合的模型在训练集和测试集上的表现都不好。

* 方差（variance）: 方差是指模型预测值与其平均预测值之间的差异。高方差意味着模型对训练数据的学习过强，它过于sensitive（敏感）地关注了训练数据中的特征，导致预测结果不稳定，从而造成过拟合（overfitting）。这种情况通常出现在过于复杂的模型中，即模型过于sensitive（敏感）地关注了训练数据中的特征，从而造成过拟合。过拟合的模型在训练集上的表现很好，但在测试集上的表现较差。

* 偏差-方差权衡（bais-variance tradeoff）: 偏差-方差权衡指的是如何平衡模型复杂度和模型的泛化能力。通过调整模型的复杂度，可以降低模型的偏差，但同时也会增加模型的方差，从而导致过拟合。通过调整模型的复杂度，可以降低模型的方差，但同时也会增加模型的偏差，从而导致欠拟合。因此，需要找到一种平衡点，使得模型的偏差和方差都能达到最低水平。
为了找到一个好的模型，我们需要找到一个平衡点，在这个点上模型既有足够的能力去学习数据中的模式（低偏差），又不会对训练数据中的随机波动过分敏感（低方差）。常见的做法包括：
1. 使用交叉验证来评估模型的不同配置。
2. 通过正则化技术来限制模型复杂度，防止过拟合。
3. 收集更多的训练数据以帮助模型更好地泛化。
4. 特征选择或特征工程，以确保输入模型的特征是有意义的，并且有助于提高模型性能。

### 解决过拟合问题
* 使用更多的训练数据
* 降低使用的特征数量，减少模型复杂度
* 使用正则化技术，如Lasso、Ridge等，限制模型的复杂度，防止过拟合

## 正则化
正则化是一种用于防止过拟合的技术，通过限制模型的复杂度，来降低模型的偏差和方差。常用的正则化技术有Lasso、Ridge等。
改变成本函数，增加正则项，实现正则化：
$$J(\theta)=\frac{1}{m}\sum_{i=1}^m[-y_i\log(h_\theta(x_i))-(1-y_i)\log(1-h_\theta(x_i))]+\lambda\sum_{j=1}^n\theta_j^2$$
正则化之后的梯度计算公式
$$ \frac{\partial J}{\partial \theta_j}=\frac{1}{m}\sum_{i=1}^m(h_\theta(x_i)-y_i)x_i^j+\lambda\theta_j$$
在实践过程中通常只会正则化$\theta_j$,而不会正则化$b$，这是因为参数$b$的正则化对模型的过拟合没有影响。
通过引入正则化项，可以在一定程度上避免过拟合。

### $\lambda$的选择
* 当$\lambda$过小时，或为0时，正则化的力度较小，模型会产生过拟合
* 当$\lambda$过大时，正则化的力度过大，模型将过于简单，模型的值会与$b$很接近，产生欠拟合
* 合理选择$\lambda$对模型正则化很重要


## 神经网络
* 神经网络与人脑
人类至今对人脑的工作方式实际上只停留在神经元传递消息上，而深度学习中使用的神经网络实际上只是模拟了人类认知范围内的神经元传递数据的方式
* 深度神经网络的兴起
研究发现，深度神经网络比起线性回归、逻辑回归这些浅层的模型能获得更好的性能


### 神经元
* 激活函数 activations
* 输入
* 输出

### 多层神经网络
又叫多层感知机
**一个神经网络需要多少层hidden layer? 每一层hidden layer需要多少个神经元? 后续需要重点学习的内容**

神经网络单个神经元节点计算公式如下：
$$y_j=\sigma(\sum_{i=1}^n w_{ji}x_i+b_j)$$

在实际的神经网络设计中，通常靠近输出端的层神经元个数越少

### 反向传播算法
反向传播算法是神经网络学习算法的核心，它通过计算梯度来更新参数，从而实现模型的优化。在反向传播算法中，损失函数的导数被传播回网络中的每个节点，以计算每个参数的梯度。然后，根据梯度的方向和大小，参数被更新，以最小化损失函数。
反向传播算法的步骤如下：
1. 前向传播：根据当前参数值，计算网络输出值。
2. 计算损失函数对参数的导数：根据当前参数值，计算损失函数对参数的导数。
3. 反向传播：根据损失函数对参数的导数，计算参数的梯度。
反向传播算法通过前向传播得出输出层l的输出值，然后计算l层的梯度下降。利用l-1层的输出，计算l-1层的梯度下降，从后往前计算每一层的梯度下降.

### 梯度下降的优化算法
* Adam优化算法
Adam算法是一种基于动量的优化算法，它结合了动量和动量优化算法，以加速模型的收敛。 Adam算法通过计算梯度的均值和方差来确定学习率，从而实现动态调整学习率。 Adam算法在处理高维数据、具有非线性ities的数据集等复杂问题的时候表现更好。
