{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 识别手写数字算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 60000\n",
      "Testing set size: 10000\n",
      "Batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 定义数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图像转换为 Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 归一化到 [-1, 1]\n",
    "])\n",
    "\n",
    "# 下载和加载训练集\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "# 下载和加载测试集\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# 检查数据集\n",
    "print(\"Training set size:\", len(trainset))\n",
    "print(\"Testing set size:\", len(testset))\n",
    "\n",
    "# 检查 DataLoader\n",
    "for images, labels in trainloader:\n",
    "    print(\"Batch shape:\", images.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    break  # 只检查第一个批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "12.4\n",
      "True\n",
      "NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/10], Loss: 0.2775\n",
      "Epoch [2/10], Loss: 0.0987\n",
      "Epoch [3/10], Loss: 0.0744\n",
      "Epoch [4/10], Loss: 0.0600\n",
      "Epoch [5/10], Loss: 0.0483\n",
      "Epoch [6/10], Loss: 0.0402\n",
      "Epoch [7/10], Loss: 0.0369\n",
      "Epoch [8/10], Loss: 0.0308\n",
      "Epoch [9/10], Loss: 0.0305\n",
      "Epoch [10/10], Loss: 0.0270\n",
      "Accuracy of the network on the 10000 test images: 98.63%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 定义数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图像转换为 Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 归一化到 [-1, 1]\n",
    "])\n",
    "\n",
    "# 使用gpu设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 下载和加载训练集\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "# 下载和加载测试集\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "# 定义卷积神经网络模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 一次卷积\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        # 第二次卷积\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        # 池化\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # 线性化\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 一次卷积、池化\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        # 第二次卷积、池化\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # 展平特征图\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        # 将数据移动到设备\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / (i+1):.4f}\")\n",
    "# 评估模型\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # 将数据移动到设备\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1.weight\n",
      "Shape: torch.Size([32, 1, 3, 3])\n",
      "Weights: tensor([[[[-0.2426, -0.4322,  0.3620],\n",
      "          [-0.3992,  0.1563,  0.3964],\n",
      "          [-0.0538,  0.4213,  0.0371]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2113, -0.0259, -0.2114],\n",
      "          [ 0.4367, -0.1806, -0.1794],\n",
      "          [ 0.4631,  0.2454, -0.3876]]],\n",
      "\n",
      "\n",
      "        [[[-0.2977,  0.0518,  0.2209],\n",
      "          [-0.3533,  0.1516,  0.1408],\n",
      "          [-0.4256,  0.3252,  0.3500]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7013, -0.3188, -0.1568],\n",
      "          [ 0.1043, -0.4721,  0.0303],\n",
      "          [ 0.2311, -0.0786,  0.1505]]],\n",
      "\n",
      "\n",
      "        [[[-0.3197, -0.3203, -0.3587],\n",
      "          [ 0.2040,  0.0571,  0.0705],\n",
      "          [ 0.2878,  0.2681,  0.2020]]],\n",
      "\n",
      "\n",
      "        [[[-0.1313, -0.0807,  0.0552],\n",
      "          [ 0.4447,  0.4402,  0.3964],\n",
      "          [-0.2656, -0.3703, -0.3862]]],\n",
      "\n",
      "\n",
      "        [[[-0.0400, -0.3263, -0.0758],\n",
      "          [ 0.1621, -0.2617, -0.2336],\n",
      "          [ 0.6032, -0.1176,  0.2780]]],\n",
      "\n",
      "\n",
      "        [[[-0.2402, -0.0969, -0.2023],\n",
      "          [ 0.1145, -0.2929, -0.0758],\n",
      "          [-0.1742, -0.2101, -0.0540]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1014,  0.1521, -0.1993],\n",
      "          [ 0.1212,  0.2267, -0.5172],\n",
      "          [ 0.0261,  0.5658, -0.2638]]],\n",
      "\n",
      "\n",
      "        [[[-0.0494,  0.1347,  0.0228],\n",
      "          [-0.2126, -0.0192,  0.0354],\n",
      "          [ 0.0854, -0.1223,  0.0298]]],\n",
      "\n",
      "\n",
      "        [[[-0.0423,  0.1732,  0.5677],\n",
      "          [ 0.0758, -0.2080,  0.0872],\n",
      "          [-0.0812, -0.2706, -0.3176]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2264,  0.4298,  0.3889],\n",
      "          [-0.0135,  0.2115,  0.1022],\n",
      "          [-0.5211, -0.4515, -0.4045]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2163, -0.2354,  0.2644],\n",
      "          [-0.3434, -0.4051,  0.1770],\n",
      "          [-0.2237, -0.0047,  0.4724]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1764, -0.2584, -0.1405],\n",
      "          [ 0.3573,  0.1404, -0.2185],\n",
      "          [-0.0563,  0.2030,  0.0994]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1450, -0.0103,  0.0555],\n",
      "          [-0.2043,  0.4013,  0.5473],\n",
      "          [-0.4060, -0.5534,  0.1122]]],\n",
      "\n",
      "\n",
      "        [[[-0.2312,  0.1058,  0.1131],\n",
      "          [ 0.2401,  0.1368,  0.0638],\n",
      "          [-0.3432,  0.0953,  0.3180]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4971, -0.2490, -0.3764],\n",
      "          [ 0.3248, -0.1291, -0.0839],\n",
      "          [ 0.0968, -0.1206,  0.1018]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0981,  0.1796, -0.4707],\n",
      "          [ 0.1044,  0.5781, -0.1422],\n",
      "          [-0.4324,  0.1241,  0.2515]]],\n",
      "\n",
      "\n",
      "        [[[-0.1754,  0.0831, -0.3407],\n",
      "          [ 0.1738, -0.0493,  0.1127],\n",
      "          [ 0.0130, -0.1276,  0.2839]]],\n",
      "\n",
      "\n",
      "        [[[-0.0261,  0.3341,  0.1394],\n",
      "          [-0.4107,  0.2948, -0.3012],\n",
      "          [-0.0991,  0.3177,  0.1856]]],\n",
      "\n",
      "\n",
      "        [[[-0.6210, -0.2758,  0.2122],\n",
      "          [ 0.2735,  0.5419,  0.0252],\n",
      "          [ 0.0956,  0.1795, -0.6287]]],\n",
      "\n",
      "\n",
      "        [[[-0.2567, -0.3350, -0.0662],\n",
      "          [-0.2566,  0.1106, -0.1321],\n",
      "          [ 0.5070,  0.2239,  0.2515]]],\n",
      "\n",
      "\n",
      "        [[[-0.2234, -0.1312, -0.0842],\n",
      "          [ 0.1436, -0.0649, -0.5066],\n",
      "          [ 0.1515,  0.5691, -0.0048]]],\n",
      "\n",
      "\n",
      "        [[[-0.3652,  0.2615,  0.2087],\n",
      "          [ 0.3617,  0.3417, -0.4949],\n",
      "          [ 0.3776, -0.1279, -0.3882]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3603,  0.3657,  0.1232],\n",
      "          [ 0.2190, -0.2154, -0.1503],\n",
      "          [-0.3079, -0.0783, -0.3757]]],\n",
      "\n",
      "\n",
      "        [[[-0.1829,  0.1569,  0.1602],\n",
      "          [ 0.2981,  0.2255, -0.2725],\n",
      "          [-0.1985,  0.0046, -0.2564]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2715, -0.5016, -0.1615],\n",
      "          [ 0.5080,  0.0813, -0.2541],\n",
      "          [ 0.1069,  0.2230,  0.5021]]],\n",
      "\n",
      "\n",
      "        [[[-0.0512,  0.2788,  0.1254],\n",
      "          [-0.3533,  0.1454,  0.1709],\n",
      "          [-0.1086,  0.0772,  0.2795]]],\n",
      "\n",
      "\n",
      "        [[[-0.4954, -0.5550,  0.1820],\n",
      "          [-0.1212,  0.1802,  0.5362],\n",
      "          [ 0.4958,  0.4051,  0.0787]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1102, -0.0752, -0.0164],\n",
      "          [ 0.3680, -0.2189,  0.1507],\n",
      "          [ 0.2762,  0.0523,  0.3718]]],\n",
      "\n",
      "\n",
      "        [[[-0.1015, -0.1839, -0.3200],\n",
      "          [ 0.1749,  0.0074, -0.3158],\n",
      "          [-0.0010,  0.1018, -0.2706]]],\n",
      "\n",
      "\n",
      "        [[[-0.3336,  0.3419,  0.4348],\n",
      "          [ 0.1882,  0.4410, -0.1636],\n",
      "          [ 0.3875,  0.1560, -0.3779]]]], device='cuda:0')\n",
      "\n",
      "Layer: conv1.bias\n",
      "Shape: torch.Size([32])\n",
      "Weights: tensor([-0.2417, -0.2807, -0.0871, -0.0977, -0.0618,  0.0535, -0.0937,  0.0073,\n",
      "        -0.1749,  0.0319, -0.0792, -0.1250, -0.2384, -0.0344, -0.0839, -0.0864,\n",
      "        -0.0663, -0.0484, -0.0450, -0.1900, -0.2369,  0.0180, -0.2241, -0.2257,\n",
      "        -0.0585, -0.0798, -0.1006,  0.0494, -0.3354, -0.0123, -0.1595, -0.0643],\n",
      "       device='cuda:0')\n",
      "\n",
      "Layer: conv2.weight\n",
      "Shape: torch.Size([64, 32, 3, 3])\n",
      "Weights: tensor([[[[-0.0316, -0.1300, -0.0217],\n",
      "          [-0.2021,  0.1125,  0.2371],\n",
      "          [ 0.0918,  0.2086, -0.0032]],\n",
      "\n",
      "         [[-0.0829, -0.2923, -0.1069],\n",
      "          [-0.2087, -0.1024,  0.1141],\n",
      "          [-0.4736,  0.0051,  0.0876]],\n",
      "\n",
      "         [[-0.0867, -0.2052, -0.0519],\n",
      "          [-0.1796,  0.1106,  0.1491],\n",
      "          [ 0.1062,  0.0887,  0.0090]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0509,  0.0944,  0.0631],\n",
      "          [ 0.0056, -0.0975, -0.0477],\n",
      "          [-0.3574,  0.0371, -0.0227]],\n",
      "\n",
      "         [[-0.0961, -0.2752, -0.2100],\n",
      "          [-0.1344, -0.2100, -0.2860],\n",
      "          [-0.1421, -0.2977, -0.2459]],\n",
      "\n",
      "         [[-0.0309, -0.0347, -0.0411],\n",
      "          [-0.0978, -0.0562,  0.2234],\n",
      "          [-0.0599,  0.1489,  0.1094]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1049,  0.1049,  0.0896],\n",
      "          [ 0.0412, -0.0717,  0.0380],\n",
      "          [-0.1531, -0.1074, -0.0203]],\n",
      "\n",
      "         [[ 0.0390,  0.1419, -0.0624],\n",
      "          [ 0.0237,  0.0727, -0.1360],\n",
      "          [ 0.0154,  0.0802,  0.0289]],\n",
      "\n",
      "         [[ 0.0685,  0.0346,  0.0553],\n",
      "          [-0.0291, -0.0910,  0.0384],\n",
      "          [-0.0771,  0.0445, -0.1522]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0027, -0.1551, -0.3749],\n",
      "          [-0.0237,  0.0505, -0.0257],\n",
      "          [ 0.0453,  0.1468,  0.0516]],\n",
      "\n",
      "         [[-0.4627, -0.1703, -0.1655],\n",
      "          [-0.3360, -0.1032, -0.1063],\n",
      "          [-0.2548, -0.1838, -0.1586]],\n",
      "\n",
      "         [[ 0.1449,  0.1513, -0.2247],\n",
      "          [ 0.1213, -0.1217, -0.0896],\n",
      "          [-0.0691,  0.0143, -0.0079]]],\n",
      "\n",
      "\n",
      "        [[[-0.0950, -0.0511, -0.0271],\n",
      "          [ 0.1048,  0.0958, -0.1325],\n",
      "          [-0.1410, -0.2335, -0.2300]],\n",
      "\n",
      "         [[-0.1624,  0.0072,  0.0056],\n",
      "          [-0.0199,  0.0764, -0.0310],\n",
      "          [-0.0606,  0.0534,  0.1183]],\n",
      "\n",
      "         [[-0.0560, -0.0311,  0.0463],\n",
      "          [-0.0134, -0.0817, -0.0608],\n",
      "          [-0.0588, -0.0320, -0.1945]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0398, -0.0150,  0.0961],\n",
      "          [ 0.1413,  0.1018, -0.1281],\n",
      "          [ 0.1306,  0.0926, -0.1486]],\n",
      "\n",
      "         [[-0.1370, -0.2891, -0.0898],\n",
      "          [-0.1205, -0.1423, -0.0758],\n",
      "          [-0.1675, -0.0801, -0.0797]],\n",
      "\n",
      "         [[ 0.0191,  0.0567,  0.0470],\n",
      "          [ 0.1389,  0.1110,  0.0789],\n",
      "          [ 0.0493,  0.0220, -0.1413]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1265, -0.3221, -0.0059],\n",
      "          [-0.0607,  0.1142,  0.1415],\n",
      "          [ 0.0383,  0.0580, -0.0833]],\n",
      "\n",
      "         [[ 0.0042,  0.0399, -0.1932],\n",
      "          [ 0.0630, -0.1362, -0.1509],\n",
      "          [ 0.0398, -0.0848, -0.0354]],\n",
      "\n",
      "         [[ 0.0168, -0.1170, -0.0063],\n",
      "          [-0.0046, -0.0263, -0.0566],\n",
      "          [ 0.0069,  0.0425, -0.0874]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0319, -0.0724, -0.0687],\n",
      "          [ 0.0304,  0.0553,  0.0701],\n",
      "          [ 0.1027,  0.1252,  0.0819]],\n",
      "\n",
      "         [[-0.0135,  0.0082,  0.0368],\n",
      "          [-0.0203, -0.0782, -0.1230],\n",
      "          [-0.0496, -0.2188, -0.2239]],\n",
      "\n",
      "         [[-0.2297, -0.1367, -0.2459],\n",
      "          [-0.0228,  0.0446,  0.0911],\n",
      "          [ 0.1875,  0.0959,  0.0925]]],\n",
      "\n",
      "\n",
      "        [[[-0.2288, -0.1486, -0.0455],\n",
      "          [-0.0309,  0.1241,  0.0275],\n",
      "          [ 0.1694,  0.1175, -0.2592]],\n",
      "\n",
      "         [[ 0.0158,  0.0380,  0.0696],\n",
      "          [-0.0754,  0.0094,  0.0614],\n",
      "          [ 0.0630,  0.0671, -0.0330]],\n",
      "\n",
      "         [[-0.0037, -0.0365, -0.0012],\n",
      "          [-0.0212,  0.0226, -0.0268],\n",
      "          [-0.0146,  0.0276, -0.1923]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0799, -0.0690, -0.0070],\n",
      "          [-0.1004, -0.1272,  0.0373],\n",
      "          [ 0.0748, -0.1301, -0.0377]],\n",
      "\n",
      "         [[-0.1017, -0.1968, -0.0778],\n",
      "          [-0.0654, -0.2021, -0.0480],\n",
      "          [-0.0535, -0.0271,  0.0102]],\n",
      "\n",
      "         [[-0.1833, -0.1434,  0.0515],\n",
      "          [-0.0201,  0.0861,  0.0809],\n",
      "          [ 0.0814,  0.1077, -0.0485]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0162, -0.0520, -0.0803],\n",
      "          [-0.0447, -0.0856, -0.3125],\n",
      "          [-0.1874, -0.0791,  0.0072]],\n",
      "\n",
      "         [[-0.0028, -0.2229, -0.0270],\n",
      "          [ 0.0786,  0.0424, -0.0357],\n",
      "          [-0.0841, -0.1152, -0.2133]],\n",
      "\n",
      "         [[-0.0161, -0.0439, -0.0839],\n",
      "          [-0.0283,  0.0110, -0.1281],\n",
      "          [-0.0229, -0.1259, -0.0751]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0174,  0.0863,  0.1084],\n",
      "          [ 0.0725,  0.0022, -0.0364],\n",
      "          [-0.0270, -0.1848, -0.1931]],\n",
      "\n",
      "         [[-0.0778, -0.0669, -0.0405],\n",
      "          [-0.0227, -0.0476, -0.1192],\n",
      "          [-0.0569, -0.0202, -0.0342]],\n",
      "\n",
      "         [[ 0.1383, -0.0764,  0.0897],\n",
      "          [-0.0709,  0.0562,  0.1322],\n",
      "          [-0.0437, -0.0491, -0.0508]]]], device='cuda:0')\n",
      "\n",
      "Layer: conv2.bias\n",
      "Shape: torch.Size([64])\n",
      "Weights: tensor([-0.0875, -0.1613, -0.0878, -0.0791, -0.0550, -0.1198, -0.0614, -0.0140,\n",
      "        -0.0880, -0.0343, -0.0679, -0.0445, -0.0153, -0.0867, -0.0422, -0.0635,\n",
      "         0.0098,  0.0132, -0.1064, -0.1184, -0.0530, -0.0145, -0.0292, -0.1151,\n",
      "        -0.0367, -0.1274, -0.0093, -0.1235, -0.0207, -0.0277, -0.0761, -0.0380,\n",
      "        -0.0272, -0.0181,  0.0047, -0.0647, -0.0176, -0.0046, -0.0487, -0.0991,\n",
      "        -0.1148, -0.1102, -0.1098, -0.1508, -0.1070, -0.0807, -0.0687, -0.0928,\n",
      "        -0.0227, -0.1227, -0.0511, -0.1031, -0.0434, -0.1421,  0.0045, -0.0373,\n",
      "        -0.0416, -0.0627, -0.1205,  0.0444, -0.1331, -0.0853, -0.0668, -0.0056],\n",
      "       device='cuda:0')\n",
      "\n",
      "Layer: fc1.weight\n",
      "Shape: torch.Size([128, 3136])\n",
      "Weights: tensor([[-0.0018, -0.0027,  0.0023,  ...,  0.0130, -0.0061,  0.0119],\n",
      "        [ 0.0020, -0.0051,  0.0048,  ..., -0.0053,  0.0049, -0.0065],\n",
      "        [ 0.0038,  0.0108,  0.0122,  ..., -0.0084,  0.0008,  0.0012],\n",
      "        ...,\n",
      "        [-0.0181,  0.0562, -0.0336,  ...,  0.0048,  0.0252,  0.0733],\n",
      "        [-0.0102, -0.0175, -0.0155,  ..., -0.0111, -0.0024, -0.0169],\n",
      "        [ 0.0060, -0.0522, -0.1160,  ...,  0.0231,  0.0625,  0.0882]],\n",
      "       device='cuda:0')\n",
      "\n",
      "Layer: fc1.bias\n",
      "Shape: torch.Size([128])\n",
      "Weights: tensor([-0.0186,  0.0034,  0.0015,  0.0011, -0.0039, -0.0238,  0.0014, -0.0029,\n",
      "         0.0102, -0.0024, -0.0205, -0.0006,  0.0034, -0.0214, -0.0207,  0.0017,\n",
      "         0.0524,  0.0039, -0.0747, -0.0129,  0.0042, -0.0504, -0.0335, -0.0724,\n",
      "        -0.0540, -0.0891,  0.0152,  0.0010, -0.0801, -0.0255, -0.0424, -0.0316,\n",
      "        -0.0032,  0.0107,  0.0067,  0.0354, -0.0449,  0.0013, -0.0386,  0.0017,\n",
      "        -0.0181, -0.0005, -0.0165, -0.0181,  0.0626, -0.0032, -0.0161, -0.0038,\n",
      "        -0.0780, -0.0230,  0.0085,  0.0439, -0.0530, -0.0168, -0.0661, -0.0209,\n",
      "        -0.0373, -0.0696, -0.0030,  0.0003, -0.0169, -0.0443, -0.0224, -0.0038,\n",
      "        -0.0081,  0.0018, -0.0190, -0.0131, -0.0656,  0.0085,  0.0047, -0.0317,\n",
      "         0.0427, -0.0174, -0.0081, -0.0358, -0.0122, -0.0125,  0.0289,  0.0012,\n",
      "        -0.0007, -0.0283, -0.0631, -0.0057, -0.0034,  0.0144, -0.0020, -0.0200,\n",
      "         0.0339, -0.0510, -0.0094, -0.0263, -0.0233, -0.0222, -0.0066, -0.0118,\n",
      "        -0.0601, -0.0184, -0.0280, -0.0947, -0.0579,  0.0025, -0.0385,  0.0059,\n",
      "        -0.0484, -0.0712, -0.0202,  0.0076,  0.0082, -0.0435,  0.0428, -0.0224,\n",
      "        -0.0012, -0.0464,  0.0073,  0.0613, -0.0099,  0.0341, -0.0109, -0.0404,\n",
      "         0.0038, -0.0032,  0.0097, -0.0147, -0.0152, -0.0064,  0.0058, -0.0313],\n",
      "       device='cuda:0')\n",
      "\n",
      "Layer: fc2.weight\n",
      "Shape: torch.Size([10, 128])\n",
      "Weights: tensor([[ 0.0629, -0.0110,  0.0596,  ..., -0.1676, -0.0047,  0.0573],\n",
      "        [-0.0228,  0.0392,  0.0293,  ...,  0.0429,  0.0034, -0.1055],\n",
      "        [ 0.0003, -0.0244, -0.0866,  ...,  0.1340, -0.0732,  0.1218],\n",
      "        ...,\n",
      "        [ 0.0555, -0.0889, -0.0760,  ..., -0.0807, -0.0076,  0.1027],\n",
      "        [ 0.0214,  0.0810,  0.0457,  ...,  0.1006, -0.0542, -0.1813],\n",
      "        [ 0.0245, -0.0805, -0.0564,  ..., -0.1042, -0.0109,  0.1068]],\n",
      "       device='cuda:0')\n",
      "\n",
      "Layer: fc2.bias\n",
      "Shape: torch.Size([10])\n",
      "Weights: tensor([ 0.1984,  0.2706, -0.0020, -0.1368, -0.0112, -0.0629,  0.0330, -0.0454,\n",
      "         0.0454, -0.0770], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 获取每一层的参数\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"Shape: {param.data.shape}\")\n",
    "        print(f\"Weights: {param.data}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper_CUDA___slow_conv2d_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpython-learning\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgeneric\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m8.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 进行预测\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m predicted_digit \u001b[38;5;241m=\u001b[39m predict_image(image_path, net, transform)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted digit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_digit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mpredict_image\u001b[1;34m(image_path, model, transform)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 使用模型进行预测\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 17\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[0;32m     18\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\python-learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\python-learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 42\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# 一次卷积、池化\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# 第二次卷积、池化\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\python-learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\python-learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\python-learning\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\python-learning\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\n\u001b[0;32m    550\u001b[0m     \u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups\n\u001b[0;32m    551\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper_CUDA___slow_conv2d_forward)"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 使用gpu设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def predict_image(image_path, model, transform):\n",
    "    # 加载图片\n",
    "    image = Image.open(image_path).convert('L')  # 转换为灰度图像\n",
    "    image = transform(image).unsqueeze(0)  # 转换为 Tensor 并增加 batch 维度\n",
    "    image.to(device)\n",
    "\n",
    "    # 使用模型进行预测\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "    \n",
    "    return predicted.item()\n",
    "\n",
    "# 指定图片路径\n",
    "image_path = 'D:\\code\\python-learning\\mnist\\generic\\8.png'\n",
    "\n",
    "# 进行预测\n",
    "predicted_digit = predict_image(image_path, net, transform)\n",
    "print(f\"Predicted digit: {predicted_digit}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.10 ('python-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "274deb4f93372eb5c9a207b9878bde76e734bb8a8f81ed86eb03424633135391"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
