{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 识别手写数字算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 60000\n",
      "Testing set size: 10000\n",
      "Batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 定义数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图像转换为 Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 归一化到 [-1, 1]\n",
    "])\n",
    "\n",
    "# 下载和加载训练集\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "# 下载和加载测试集\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# 检查数据集\n",
    "print(\"Training set size:\", len(trainset))\n",
    "print(\"Testing set size:\", len(testset))\n",
    "\n",
    "# 检查 DataLoader\n",
    "for images, labels in trainloader:\n",
    "    print(\"Batch shape:\", images.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    break  # 只检查第一个批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "12.4\n",
      "True\n",
      "NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/10], Loss: 0.3131\n",
      "Epoch [2/10], Loss: 0.1147\n",
      "Epoch [3/10], Loss: 0.0846\n",
      "Epoch [4/10], Loss: 0.0694\n",
      "Epoch [5/10], Loss: 0.0578\n",
      "Epoch [6/10], Loss: 0.0511\n",
      "Epoch [7/10], Loss: 0.0449\n",
      "Epoch [8/10], Loss: 0.0365\n",
      "Epoch [9/10], Loss: 0.0348\n",
      "Epoch [10/10], Loss: 0.0290\n",
      "Accuracy of the network on the 10000 test images: 98.63%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 定义数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图像转换为 Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 归一化到 [-1, 1]\n",
    "])\n",
    "\n",
    "# 使用gpu设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 下载和加载训练集\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "# 下载和加载测试集\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "# 定义卷积神经网络模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 一次卷积\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        # 第二次卷积\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        # 池化\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # 线性化\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 一次卷积、池化\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        # 第二次卷积、池化\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # 展平特征图\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        # 将数据移动到设备\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / (i+1):.4f}\")\n",
    "# 评估模型\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # 将数据移动到设备\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1.weight\n",
      "Shape: torch.Size([32, 1, 3, 3])\n",
      "Weights: tensor([[[[-1.5241e-01,  1.9503e-01,  2.9506e-01],\n",
      "          [-3.4260e-01,  2.2655e-01, -3.2656e-03],\n",
      "          [ 2.4507e-01,  3.7953e-01, -2.2848e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.9374e-01, -3.6419e-01, -3.7965e-01],\n",
      "          [ 2.2273e-01, -2.7766e-01,  2.3187e-01],\n",
      "          [ 2.0008e-01,  5.8296e-01, -7.4937e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.1235e-01, -4.6888e-01, -2.5889e-01],\n",
      "          [-3.3509e-01,  1.5747e-01, -1.4213e-01],\n",
      "          [ 4.8283e-01,  4.0216e-01,  3.5739e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8435e-01,  2.2883e-01, -3.8671e-01],\n",
      "          [-2.5230e-01,  6.9204e-01,  3.5845e-01],\n",
      "          [-5.2684e-01, -3.5736e-01,  3.7743e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0163e-01, -1.0509e-01,  4.4767e-01],\n",
      "          [-1.8327e-01, -8.2945e-03,  8.3378e-02],\n",
      "          [-2.3038e-01, -1.0218e-01,  3.8530e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.0703e-01, -7.5556e-02,  2.2841e-01],\n",
      "          [-2.5348e-01,  5.9550e-01,  1.6105e-01],\n",
      "          [ 3.1421e-01,  3.2434e-01, -6.4775e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.3666e-02,  1.8903e-01, -3.3020e-02],\n",
      "          [ 1.1364e-01,  3.2551e-01,  5.3961e-01],\n",
      "          [-3.7474e-01, -6.2066e-01,  1.2472e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.1256e-04,  4.6017e-01, -3.0245e-01],\n",
      "          [-1.7497e-01,  2.9096e-01,  1.3037e-01],\n",
      "          [-2.2290e-01,  2.3760e-01,  2.2521e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.1905e-02,  4.9038e-02, -6.6040e-02],\n",
      "          [-1.4294e-01, -4.2329e-01,  1.8090e-01],\n",
      "          [-1.7027e-01,  9.4533e-02,  4.6419e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.5556e-01,  6.2876e-02, -2.4970e-01],\n",
      "          [ 1.8314e-01, -2.3951e-01, -4.4069e-02],\n",
      "          [ 3.7210e-01, -1.4215e-01, -2.5684e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8509e-01,  2.0407e-01,  5.1012e-01],\n",
      "          [ 2.2680e-01,  4.2745e-01,  1.1701e-01],\n",
      "          [ 2.5564e-01, -2.9844e-01, -5.6816e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 5.4913e-01,  2.0466e-02, -1.2111e-01],\n",
      "          [-1.2998e-01,  3.7024e-01,  5.0078e-02],\n",
      "          [ 2.0308e-01,  4.4895e-01,  2.5692e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.7150e-01,  4.1860e-01,  8.6159e-02],\n",
      "          [-1.6195e-01,  3.3472e-01,  1.6119e-01],\n",
      "          [-4.8522e-01, -1.4212e-01,  2.7332e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.3868e-01,  2.6985e-01,  2.6967e-01],\n",
      "          [ 2.1502e-01, -9.2927e-02, -2.6834e-01],\n",
      "          [-2.3094e-01, -1.9880e-01, -4.5667e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.7204e-02, -3.1573e-01,  1.1325e-01],\n",
      "          [ 1.8581e-01,  4.1584e-01,  2.5506e-01],\n",
      "          [ 1.8381e-02, -3.1584e-01, -3.3414e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.8354e-02,  2.7810e-01,  5.0378e-01],\n",
      "          [-4.3184e-01, -4.2535e-01, -9.1432e-02],\n",
      "          [ 6.4604e-02, -3.6133e-01, -2.2756e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.2442e-01, -3.5289e-01, -3.8060e-01],\n",
      "          [-1.7111e-01, -1.2620e-01,  2.3547e-01],\n",
      "          [ 2.2718e-02, -4.6595e-02,  4.6831e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.2240e-01, -3.2968e-01, -6.5407e-01],\n",
      "          [ 5.3452e-01,  3.1740e-01,  2.1131e-01],\n",
      "          [-6.3313e-02,  1.2945e-01,  4.2792e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3439e-01, -4.8573e-01,  1.3838e-01],\n",
      "          [ 4.6607e-03, -5.9181e-01, -2.7160e-01],\n",
      "          [ 5.9875e-01,  3.9794e-01,  1.7403e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.7368e-01,  3.9848e-01,  2.0234e-01],\n",
      "          [-5.1976e-02,  1.1503e-01, -8.2839e-02],\n",
      "          [ 2.2497e-01,  3.2971e-01,  2.5200e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0774e-01,  2.2967e-02,  2.0760e-01],\n",
      "          [-9.0093e-02,  5.1160e-01, -1.9492e-01],\n",
      "          [-3.8807e-01,  1.5466e-01, -4.4519e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9849e-02, -4.5173e-01, -2.0974e-01],\n",
      "          [-4.0820e-01, -4.4066e-02,  5.0126e-01],\n",
      "          [-1.6949e-01,  3.7968e-01,  2.8146e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4571e-01,  1.3230e-02,  3.9169e-02],\n",
      "          [ 1.1390e-01,  9.0479e-03,  1.5035e-01],\n",
      "          [-5.8352e-02, -1.1511e-01, -3.7326e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.5036e-03, -5.7145e-01, -2.0959e-01],\n",
      "          [ 3.8459e-01,  4.4706e-01,  1.1067e-01],\n",
      "          [ 1.0307e-01, -3.1968e-01,  1.3089e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.9966e-01, -9.8938e-02,  4.1073e-01],\n",
      "          [ 4.1593e-01,  3.5382e-01, -2.0309e-01],\n",
      "          [-1.8256e-01, -2.7118e-01, -2.2369e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.0143e-01,  3.6422e-01, -4.1814e-01],\n",
      "          [-4.8322e-02,  1.5714e-01, -5.0950e-01],\n",
      "          [ 2.2134e-01,  2.4436e-01,  1.7562e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.6702e-01, -5.0394e-01, -2.3210e-01],\n",
      "          [ 4.7751e-01,  7.0758e-02,  2.8641e-01],\n",
      "          [ 2.4524e-01,  1.4823e-01,  3.0789e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3045e-01, -3.3507e-01, -2.9991e-01],\n",
      "          [ 4.6196e-01, -2.6640e-02, -1.0169e-01],\n",
      "          [-1.4631e-01,  2.0130e-01, -1.4622e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.2031e-01,  1.9183e-01,  2.4800e-01],\n",
      "          [-2.8829e-01, -4.2962e-02, -3.0415e-01],\n",
      "          [ 1.8991e-01,  1.9166e-01, -1.3277e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.0273e-01, -2.2746e-02, -1.1406e-01],\n",
      "          [-1.0683e-01, -2.9205e-01, -3.1014e-01],\n",
      "          [-2.3529e-01, -6.8556e-03, -2.7367e-01]]],\n",
      "\n",
      "\n",
      "        [[[-6.6475e-02, -1.4003e-01, -2.4799e-01],\n",
      "          [-2.8313e-01, -3.8134e-01, -2.1430e-01],\n",
      "          [-2.6442e-01,  2.0958e-01,  1.9219e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8049e-01,  4.4794e-01,  1.5193e-01],\n",
      "          [-1.6752e-02,  1.2764e-01,  2.7892e-01],\n",
      "          [-4.8859e-01, -5.6730e-01, -2.8649e-01]]]], device='cuda:0')\n",
      "\n",
      "Layer: conv1.bias\n",
      "Shape: torch.Size([32])\n",
      "Weights: tensor([-0.0038, -0.1771, -0.1215, -0.1275, -0.1638, -0.0282, -0.1686,  0.1319,\n",
      "        -0.0992,  0.1068, -0.1644, -0.2432, -0.0356, -0.1079, -0.0665, -0.3479,\n",
      "        -0.3857, -0.2197, -0.0871, -0.1664, -0.1747, -0.1954,  0.0019, -0.0826,\n",
      "        -0.0989, -0.0029, -0.2001, -0.1159, -0.1336,  0.2050, -0.2921, -0.0640],\n",
      "       device='cuda:0')\n",
      "\n",
      "Layer: conv2.weight\n",
      "Shape: torch.Size([64, 32, 3, 3])\n",
      "Weights: tensor([[[[ 0.0786, -0.2853, -0.2525],\n",
      "          [-0.1027, -0.1303, -0.1769],\n",
      "          [ 0.0270, -0.0327, -0.0789]],\n",
      "\n",
      "         [[-0.1664,  0.0606,  0.1839],\n",
      "          [ 0.0518,  0.2042,  0.1411],\n",
      "          [-0.1185, -0.2350, -0.3064]],\n",
      "\n",
      "         [[-0.0937,  0.1170,  0.1950],\n",
      "          [-0.0434,  0.1672,  0.0052],\n",
      "          [-0.1401, -0.1696, -0.1265]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1785, -0.1571, -0.0396],\n",
      "          [-0.1480, -0.1319, -0.0508],\n",
      "          [-0.1536, -0.0649, -0.1488]],\n",
      "\n",
      "         [[-0.2512, -0.0826, -0.0086],\n",
      "          [-0.1319, -0.1399, -0.0284],\n",
      "          [-0.0658, -0.0601, -0.1013]],\n",
      "\n",
      "         [[-0.0840,  0.0574, -0.0259],\n",
      "          [-0.0051,  0.1134,  0.1512],\n",
      "          [-0.0147,  0.0092, -0.0362]]],\n",
      "\n",
      "\n",
      "        [[[-0.0650, -0.0318,  0.1655],\n",
      "          [-0.0792,  0.0760,  0.0610],\n",
      "          [-0.1851, -0.2581, -0.0914]],\n",
      "\n",
      "         [[ 0.0295,  0.0680, -0.0528],\n",
      "          [-0.2275, -0.0343, -0.2656],\n",
      "          [ 0.1039, -0.2034, -0.3572]],\n",
      "\n",
      "         [[ 0.0484,  0.0468, -0.0635],\n",
      "          [-0.2562, -0.1319, -0.1260],\n",
      "          [-0.0536, -0.3555, -0.2683]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0660, -0.0091,  0.0107],\n",
      "          [ 0.0260, -0.0655,  0.0142],\n",
      "          [ 0.0594, -0.0505,  0.0067]],\n",
      "\n",
      "         [[-0.0793,  0.0451,  0.0902],\n",
      "          [-0.0364, -0.0762, -0.0614],\n",
      "          [-0.0151, -0.0304, -0.0258]],\n",
      "\n",
      "         [[ 0.0633,  0.0151,  0.0166],\n",
      "          [ 0.1901,  0.0406,  0.0233],\n",
      "          [-0.0363,  0.1024, -0.0327]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0727,  0.0936, -0.0458],\n",
      "          [ 0.0242, -0.0013, -0.1347],\n",
      "          [-0.0182, -0.0172, -0.0907]],\n",
      "\n",
      "         [[-0.3044, -0.2984, -0.2637],\n",
      "          [-0.3687, -0.1254,  0.0635],\n",
      "          [-0.0427,  0.0217,  0.1235]],\n",
      "\n",
      "         [[-0.3504, -0.3717, -0.3504],\n",
      "          [-0.1532, -0.1179,  0.0481],\n",
      "          [ 0.1029, -0.0209,  0.1150]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0670, -0.1729, -0.1148],\n",
      "          [-0.0033, -0.0392, -0.0585],\n",
      "          [ 0.0366,  0.0394,  0.0526]],\n",
      "\n",
      "         [[-0.1177, -0.2182, -0.1521],\n",
      "          [ 0.0065, -0.1254, -0.0217],\n",
      "          [-0.0079, -0.0662, -0.0799]],\n",
      "\n",
      "         [[-0.0838, -0.1100,  0.1073],\n",
      "          [-0.0911,  0.0501, -0.1321],\n",
      "          [ 0.0109,  0.0368,  0.0870]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0483,  0.0892,  0.0292],\n",
      "          [-0.0499,  0.0172, -0.0186],\n",
      "          [-0.0317, -0.1565, -0.1384]],\n",
      "\n",
      "         [[-0.0237,  0.0005,  0.0119],\n",
      "          [ 0.0038, -0.0071,  0.0749],\n",
      "          [-0.0258, -0.1002,  0.1994]],\n",
      "\n",
      "         [[ 0.0250, -0.0598,  0.0493],\n",
      "          [-0.0383, -0.0773, -0.0351],\n",
      "          [-0.0084, -0.1066, -0.0719]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0869, -0.1077, -0.1293],\n",
      "          [-0.0662, -0.1045, -0.0753],\n",
      "          [-0.1598, -0.2082, -0.1597]],\n",
      "\n",
      "         [[-0.0349, -0.0485, -0.0712],\n",
      "          [-0.0958, -0.0833, -0.0366],\n",
      "          [-0.0726, -0.3596, -0.2739]],\n",
      "\n",
      "         [[-0.0443, -0.0368,  0.1465],\n",
      "          [-0.1087, -0.1123, -0.0491],\n",
      "          [ 0.0738,  0.0658, -0.0396]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0554, -0.0215, -0.0250],\n",
      "          [ 0.0178,  0.0529,  0.0125],\n",
      "          [-0.0963,  0.0111, -0.0135]],\n",
      "\n",
      "         [[ 0.0929, -0.1479,  0.1005],\n",
      "          [ 0.0863,  0.0251, -0.1147],\n",
      "          [-0.0729,  0.0169, -0.0103]],\n",
      "\n",
      "         [[ 0.0152, -0.1840, -0.1165],\n",
      "          [-0.0538,  0.1146, -0.0876],\n",
      "          [-0.1224,  0.0019,  0.0212]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1781, -0.1337, -0.0565],\n",
      "          [-0.2573, -0.2012, -0.0954],\n",
      "          [-0.1465, -0.1618, -0.1191]],\n",
      "\n",
      "         [[-0.1678, -0.2014, -0.1204],\n",
      "          [-0.1977, -0.1931, -0.0643],\n",
      "          [-0.0925, -0.1686,  0.0128]],\n",
      "\n",
      "         [[-0.0055,  0.1621, -0.1428],\n",
      "          [-0.1282, -0.2074, -0.0111],\n",
      "          [ 0.0106, -0.0133, -0.0160]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0773, -0.0681,  0.0149],\n",
      "          [-0.1845, -0.1010, -0.0247],\n",
      "          [-0.1474, -0.0649,  0.0035]],\n",
      "\n",
      "         [[-0.0453, -0.0155,  0.0017],\n",
      "          [ 0.0826, -0.0326,  0.0150],\n",
      "          [-0.0614,  0.0408, -0.2127]],\n",
      "\n",
      "         [[-0.0209, -0.0052,  0.0054],\n",
      "          [ 0.0080, -0.0496,  0.0364],\n",
      "          [ 0.0836,  0.0264, -0.0905]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0010, -0.0645, -0.0358],\n",
      "          [-0.0447, -0.0667, -0.0737],\n",
      "          [ 0.0410,  0.0080, -0.0337]],\n",
      "\n",
      "         [[ 0.0161, -0.0159,  0.0416],\n",
      "          [-0.1206, -0.1281, -0.0192],\n",
      "          [ 0.0319, -0.0473,  0.0471]],\n",
      "\n",
      "         [[-0.0699, -0.2834, -0.4754],\n",
      "          [ 0.0686, -0.0570, -0.0475],\n",
      "          [ 0.2242,  0.1678,  0.1288]]]], device='cuda:0')\n",
      "\n",
      "Layer: conv2.bias\n",
      "Shape: torch.Size([64])\n",
      "Weights: tensor([-0.1222, -0.0364, -0.0374, -0.1253, -0.0739, -0.0630, -0.0810, -0.1007,\n",
      "        -0.0595, -0.0234, -0.1062, -0.0833, -0.0615, -0.0393, -0.0035, -0.0359,\n",
      "        -0.1109, -0.0446, -0.0223, -0.0404, -0.0236,  0.0106, -0.0047, -0.0476,\n",
      "        -0.0652,  0.0142,  0.0436,  0.0193, -0.0336, -0.0544, -0.1124, -0.0444,\n",
      "        -0.0561,  0.0259, -0.1319, -0.0242, -0.0502,  0.0270, -0.0042, -0.0829,\n",
      "         0.0007, -0.0810, -0.0004, -0.0716, -0.0956, -0.0314, -0.0199, -0.1002,\n",
      "        -0.1375,  0.0168, -0.0329, -0.1157, -0.0208,  0.0271, -0.1352, -0.0497,\n",
      "        -0.0326, -0.0354, -0.0168, -0.0700, -0.0310, -0.0225, -0.0296, -0.0127],\n",
      "       device='cuda:0')\n",
      "\n",
      "Layer: fc1.weight\n",
      "Shape: torch.Size([128, 3136])\n",
      "Weights: tensor([[ 0.0026, -0.0026,  0.0454,  ...,  0.0840, -0.0909, -0.0560],\n",
      "        [-0.0026, -0.0098, -0.0173,  ..., -0.0068, -0.0084, -0.0054],\n",
      "        [ 0.0556,  0.0003, -0.0351,  ...,  0.0150,  0.1048, -0.0585],\n",
      "        ...,\n",
      "        [-0.0156, -0.0147, -0.0185,  ...,  0.0255, -0.0042, -0.0061],\n",
      "        [-0.0107, -0.0150,  0.0211,  ..., -0.0374,  0.0292, -0.0139],\n",
      "        [-0.0109, -0.0128,  0.0105,  ..., -0.0328,  0.0378,  0.0110]],\n",
      "       device='cuda:0')\n",
      "\n",
      "Layer: fc1.bias\n",
      "Shape: torch.Size([128])\n",
      "Weights: tensor([-0.0582, -0.0040, -0.0076, -0.0221, -0.0121, -0.0087,  0.0116, -0.0154,\n",
      "         0.0311, -0.0580,  0.0091, -0.0084, -0.0496,  0.0070, -0.0405, -0.0870,\n",
      "        -0.0224, -0.0472,  0.0264, -0.0009, -0.0163,  0.0144, -0.0078, -0.0063,\n",
      "         0.0020, -0.0790, -0.0135,  0.0196, -0.0283, -0.0226,  0.0083, -0.0038,\n",
      "        -0.0039, -0.0734, -0.0070, -0.0236, -0.0121, -0.0108, -0.0404, -0.0697,\n",
      "        -0.0512,  0.0042, -0.0393,  0.0100, -0.0199, -0.0678, -0.0122, -0.0228,\n",
      "        -0.0273, -0.0305, -0.0163, -0.0210, -0.0245,  0.0281, -0.0416, -0.0232,\n",
      "        -0.0096, -0.0661, -0.0197, -0.0216, -0.0216, -0.0117, -0.0376, -0.0189,\n",
      "        -0.0010, -0.0457, -0.0606, -0.0299, -0.0197, -0.0435,  0.0027, -0.0112,\n",
      "         0.0065, -0.0136, -0.0768, -0.0038, -0.0192,  0.0099,  0.0696,  0.0301,\n",
      "        -0.0019, -0.0005,  0.0026,  0.0023, -0.0272, -0.0914,  0.0315, -0.0109,\n",
      "        -0.0215,  0.0791, -0.0172, -0.0007, -0.0541,  0.0088, -0.0121, -0.0583,\n",
      "         0.0002, -0.0515, -0.0389, -0.0202,  0.0040, -0.0181, -0.0155, -0.0382,\n",
      "         0.0229, -0.0048,  0.0075, -0.0500, -0.0212,  0.0602, -0.0367,  0.0481,\n",
      "        -0.0099,  0.0082, -0.0393, -0.0479,  0.0114, -0.0328, -0.0187,  0.0304,\n",
      "        -0.0033, -0.1217, -0.0171, -0.0193,  0.0029, -0.0297, -0.0558, -0.0255],\n",
      "       device='cuda:0')\n",
      "\n",
      "Layer: fc2.weight\n",
      "Shape: torch.Size([10, 128])\n",
      "Weights: tensor([[-0.2079, -0.0856, -0.3083,  ..., -0.0152, -0.0343, -0.0678],\n",
      "        [-0.0421, -0.0066,  0.0715,  ...,  0.0890, -0.1891, -0.0839],\n",
      "        [ 0.1176, -0.0198,  0.0186,  ...,  0.0631,  0.0642, -0.1114],\n",
      "        ...,\n",
      "        [ 0.1269, -0.0223,  0.0743,  ..., -0.0341,  0.0638,  0.0509],\n",
      "        [-0.1208, -0.0229, -0.1318,  ..., -0.0808,  0.0654, -0.1158],\n",
      "        [ 0.1009,  0.0506,  0.0810,  ..., -0.0556,  0.0818, -0.1211]],\n",
      "       device='cuda:0')\n",
      "\n",
      "Layer: fc2.bias\n",
      "Shape: torch.Size([10])\n",
      "Weights: tensor([ 0.0644,  0.1832, -0.0415, -0.0007, -0.1502, -0.0869, -0.2634, -0.0824,\n",
      "         0.0703,  0.0472], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 获取每一层的参数\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"Shape: {param.data.shape}\")\n",
    "        print(f\"Weights: {param.data}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 5\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 7\n",
      "Predicted digit: 5\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 7\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 9\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 0\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 9\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 5\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 9\n",
      "Predicted digit: 3\n",
      "Predicted digit: 3\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 5\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 5\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 5\n",
      "Predicted digit: 7\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 9\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 9\n",
      "Predicted digit: 9\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 5\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 9\n",
      "Predicted digit: 5\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "Predicted digit: 8\n",
      "The value 7 appears 3 times in the list.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 使用gpu设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def predict_image(image_path, model, transform):\n",
    "    # 加载图片\n",
    "    image = Image.open(image_path).convert('L')  # 转换为灰度图像\n",
    "    image = transform(image).unsqueeze(0)  # 转换为 Tensor 并增加 batch 维度\n",
    "    image = image.to(device)\n",
    "\n",
    "    # 使用模型进行预测\n",
    "    with torch.no_grad():\n",
    "        model.to(device)\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "    \n",
    "    return predicted.item()\n",
    "\n",
    "# 指定图片路径\n",
    "image_path = 'D:\\code\\python-learning\\mnist\\generic\\\\7.png'\n",
    "\n",
    "# 进行预测\n",
    "result = []\n",
    "for i in range(100):\n",
    "    predicted_digit = predict_image(image_path, net, transform)\n",
    "    print(f\"Predicted digit: {predicted_digit}\")\n",
    "    result.append(predicted_digit)\n",
    "value_to_count = 7\n",
    "count = result.count(value_to_count)\n",
    "print(f\"The value {value_to_count} appears {count} times in the list.\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.10 ('python-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "274deb4f93372eb5c9a207b9878bde76e734bb8a8f81ed86eb03424633135391"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
